{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LittleVGG for Emotion Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Emotion Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28273 images belonging to 6 classes.\n",
      "Found 3534 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "\n",
    "num_classes = 6\n",
    "img_rows, img_cols = 32, 32\n",
    "batch_size = 32\n",
    "\n",
    "train_data_dir = 'train'\n",
    "validation_data_dir = 'validation'\n",
    "\n",
    "# Let's use some data augmentaiton \n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=30,\n",
    "      shear_range=0.3,\n",
    "      zoom_range=0.3,\n",
    "      width_shift_range=0.4,\n",
    "      height_shift_range=0.4,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    " \n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    " \n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        color_mode = 'grayscale',\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True)\n",
    " \n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        color_mode = 'grayscale',\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Keras Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.layers.core import Activation, Flatten, Dropout, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras LittleVGG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_41 (Conv2D)           (None, 32, 32, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_49 (Batc (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_42 (Conv2D)           (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_50 (Batc (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_43 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_51 (Batc (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_44 (Conv2D)           (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_52 (Batc (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_45 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_53 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_46 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_54 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_47 (Conv2D)           (None, 4, 4, 256)         295168    \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_55 (Batc (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_48 (Conv2D)           (None, 4, 4, 256)         590080    \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_56 (Batc (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_49 (Conv2D)           (None, 2, 2, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_57 (Batc (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv2d_50 (Conv2D)           (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_58 (Batc (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_59 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_60 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 6)                 390       \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 4,757,478\n",
      "Trainable params: 4,753,254\n",
      "Non-trainable params: 4,224\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding = 'same', kernel_initializer=\"he_normal\",\n",
    "                 input_shape = (img_rows, img_cols, 1)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3, 3), padding = \"same\", kernel_initializer=\"he_normal\", \n",
    "                 input_shape = (img_rows, img_cols, 1)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block #2: second CONV => RELU => CONV => RELU => POOL\n",
    "# layer set\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block #3: third CONV => RELU => CONV => RELU => POOL\n",
    "# layer set\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block #4: third CONV => RELU => CONV => RELU => POOL\n",
    "# layer set\n",
    "model.add(Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block #: third CONV => RELU => CONV => RELU => POOL\n",
    "# layer set\n",
    "model.add(Conv2D(512, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(512, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block #5: first set of FC => RELU layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Block #6: second set of FC => RELU layers\n",
    "model.add(Dense(64, kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Block #7: softmax classifier\n",
    "model.add(Dense(num_classes, kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "883/883 [==============================] - 1143s 1s/step - loss: 1.8034 - acc: 0.2382 - val_loss: 1.7450 - val_acc: 0.2491\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.74502, saving model to emotion_little_vgg_3.h5\n",
      "Epoch 2/30\n",
      "883/883 [==============================] - 1045s 1s/step - loss: 1.7409 - acc: 0.2580 - val_loss: 1.7632 - val_acc: 0.2444\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.74502\n",
      "Epoch 3/30\n",
      "883/883 [==============================] - 1091s 1s/step - loss: 1.7415 - acc: 0.2563 - val_loss: 1.7742 - val_acc: 0.2287\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.74502\n",
      "Epoch 4/30\n",
      "883/883 [==============================] - 1159s 1s/step - loss: 1.7402 - acc: 0.2551 - val_loss: 1.7416 - val_acc: 0.2524\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.74502 to 1.74156, saving model to emotion_little_vgg_3.h5\n",
      "Epoch 5/30\n",
      "883/883 [==============================] - 1164s 1s/step - loss: 1.7341 - acc: 0.2575 - val_loss: 1.7482 - val_acc: 0.2533\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.74156\n",
      "Epoch 6/30\n",
      "883/883 [==============================] - 1149s 1s/step - loss: 1.7181 - acc: 0.2697 - val_loss: 1.6690 - val_acc: 0.2961\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.74156 to 1.66895, saving model to emotion_little_vgg_3.h5\n",
      "Epoch 7/30\n",
      "883/883 [==============================] - 1148s 1s/step - loss: 1.7009 - acc: 0.2809 - val_loss: 1.7691 - val_acc: 0.2901\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.66895\n",
      "Epoch 8/30\n",
      "883/883 [==============================] - 1159s 1s/step - loss: 1.6672 - acc: 0.2963 - val_loss: 1.6263 - val_acc: 0.3095\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.66895 to 1.62628, saving model to emotion_little_vgg_3.h5\n",
      "Epoch 9/30\n",
      "883/883 [==============================] - 1150s 1s/step - loss: 1.6360 - acc: 0.3174 - val_loss: 1.5880 - val_acc: 0.3469\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.62628 to 1.58797, saving model to emotion_little_vgg_3.h5\n",
      "Epoch 10/30\n",
      "883/883 [==============================] - 1156s 1s/step - loss: 1.5956 - acc: 0.3387 - val_loss: 1.6151 - val_acc: 0.3635\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.58797\n",
      "Epoch 11/30\n",
      "883/883 [==============================] - 1160s 1s/step - loss: 1.5727 - acc: 0.3545 - val_loss: 1.5186 - val_acc: 0.3812\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.58797 to 1.51856, saving model to emotion_little_vgg_3.h5\n",
      "Epoch 12/30\n",
      "883/883 [==============================] - 1159s 1s/step - loss: 1.5570 - acc: 0.3611 - val_loss: 1.5502 - val_acc: 0.3718\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.51856\n",
      "Epoch 13/30\n",
      "883/883 [==============================] - 1154s 1s/step - loss: 1.5450 - acc: 0.3699 - val_loss: 1.5834 - val_acc: 0.4026\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.51856\n",
      "Epoch 14/30\n",
      "883/883 [==============================] - 1157s 1s/step - loss: 1.5341 - acc: 0.3747 - val_loss: 1.5046 - val_acc: 0.4183\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.51856 to 1.50456, saving model to emotion_little_vgg_3.h5\n",
      "Epoch 15/30\n",
      "883/883 [==============================] - 1159s 1s/step - loss: 1.5222 - acc: 0.3836 - val_loss: 1.4595 - val_acc: 0.4061\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.50456 to 1.45953, saving model to emotion_little_vgg_3.h5\n",
      "Epoch 16/30\n",
      "883/883 [==============================] - 1160s 1s/step - loss: 1.5212 - acc: 0.3815 - val_loss: 1.5081 - val_acc: 0.4069\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.45953\n",
      "Epoch 17/30\n",
      "883/883 [==============================] - 1159s 1s/step - loss: 1.5057 - acc: 0.3894 - val_loss: 1.6345 - val_acc: 0.3726\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.45953\n",
      "Epoch 18/30\n",
      "883/883 [==============================] - 1159s 1s/step - loss: 1.4994 - acc: 0.3881 - val_loss: 1.4413 - val_acc: 0.4249\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.45953 to 1.44126, saving model to emotion_little_vgg_3.h5\n",
      "Epoch 19/30\n",
      "883/883 [==============================] - 1161s 1s/step - loss: 1.4843 - acc: 0.3951 - val_loss: 1.4596 - val_acc: 0.4026\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.44126\n",
      "Epoch 20/30\n",
      "883/883 [==============================] - 1163s 1s/step - loss: 1.4830 - acc: 0.3987 - val_loss: 1.4042 - val_acc: 0.4358\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.44126 to 1.40420, saving model to emotion_little_vgg_3.h5\n",
      "Epoch 21/30\n",
      "883/883 [==============================] - 1164s 1s/step - loss: 1.4769 - acc: 0.4015 - val_loss: 1.4262 - val_acc: 0.4386\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.40420\n",
      "Epoch 22/30\n",
      "883/883 [==============================] - 1169s 1s/step - loss: 1.4759 - acc: 0.4037 - val_loss: 1.4817 - val_acc: 0.4372\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.40420\n",
      "Epoch 23/30\n",
      "883/883 [==============================] - 1167s 1s/step - loss: 1.4682 - acc: 0.4091 - val_loss: 1.4235 - val_acc: 0.4429\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.40420\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "Epoch 00023: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import RMSprop, SGD, Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "                     \n",
    "checkpoint = ModelCheckpoint(\"emotion_little_vgg_3.h5\",\n",
    "                             monitor=\"val_loss\",\n",
    "                             mode=\"min\",\n",
    "                             save_best_only = True,\n",
    "                             verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', \n",
    "                          min_delta = 0, \n",
    "                          patience = 3,\n",
    "                          verbose = 1,\n",
    "                          restore_best_weights = True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, patience = 3, verbose = 1, min_delta = 0.0001)\n",
    "\n",
    "# we put our call backs into a callback list\n",
    "callbacks = [earlystop, checkpoint, reduce_lr]\n",
    "\n",
    "# We use a very small learning rate \n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = Adam(lr=0.01),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "nb_train_samples = 28273\n",
    "nb_validation_samples = 3534\n",
    "epochs = 30\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = nb_train_samples // batch_size,\n",
    "    epochs = epochs,\n",
    "    callbacks = callbacks,\n",
    "    validation_data = validation_generator,\n",
    "    validation_steps = nb_validation_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3534 images belonging to 6 classes.\n",
      "Confusion Matrix\n",
      "[[  0  44  85 179 148  35]\n",
      " [  0  68  90 152 130  88]\n",
      " [  0  13 763  12  74  17]\n",
      " [  0  41 188 194 127  76]\n",
      " [  0  28  96 231 222  17]\n",
      " [  0  26  64  31  12 283]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Angry       0.00      0.00      0.00       491\n",
      "        Fear       0.31      0.13      0.18       528\n",
      "       Happy       0.59      0.87      0.70       879\n",
      "     Neutral       0.24      0.31      0.27       626\n",
      "         Sad       0.31      0.37      0.34       594\n",
      "    Surprise       0.55      0.68      0.61       416\n",
      "\n",
      "   micro avg       0.43      0.43      0.43      3534\n",
      "   macro avg       0.33      0.39      0.35      3534\n",
      "weighted avg       0.35      0.43      0.38      3534\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abuzaid/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "nb_train_samples = 28273\n",
    "nb_validation_samples = 3534\n",
    "\n",
    "# We need to recreate our validation generator with shuffle = false\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        color_mode = 'grayscale',\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "class_labels = validation_generator.class_indices\n",
    "class_labels = {v: k for k, v in class_labels.items()}\n",
    "classes = list(class_labels.values())\n",
    "\n",
    "#Confution Matrix and Classification Report\n",
    "Y_pred = model.predict_generator(validation_generator, nb_validation_samples // batch_size+1)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(validation_generator.classes, y_pred))\n",
    "print('Classification Report')\n",
    "target_names = list(class_labels.values())\n",
    "print(classification_report(validation_generator.classes, y_pred, target_names=target_names))\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "cnf_matrix = confusion_matrix(validation_generator.classes, y_pred)\n",
    "\n",
    "plt.imshow(cnf_matrix, interpolation='nearest')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(classes))\n",
    "_ = plt.xticks(tick_marks, classes, rotation=90)\n",
    "_ = plt.yticks(tick_marks, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading our saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0614 14:39:38.840416 4609353152 deprecation_wrapper.py:119] From /Users/abuzaid/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0614 14:39:38.870368 4609353152 deprecation_wrapper.py:119] From /Users/abuzaid/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "W0614 14:39:38.943194 4609353152 deprecation_wrapper.py:119] From /Users/abuzaid/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:245: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0614 14:39:38.945096 4609353152 deprecation_wrapper.py:119] From /Users/abuzaid/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0614 14:39:38.949159 4609353152 deprecation_wrapper.py:119] From /Users/abuzaid/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0614 14:39:38.989855 4609353152 deprecation_wrapper.py:119] From /Users/abuzaid/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "W0614 14:39:39.209675 4609353152 deprecation_wrapper.py:119] From /Users/abuzaid/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0614 14:39:39.306494 4609353152 deprecation.py:506] From /Users/abuzaid/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0614 14:39:43.140092 4609353152 deprecation_wrapper.py:119] From /Users/abuzaid/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0614 14:39:43.336638 4609353152 deprecation.py:323] From /Users/abuzaid/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "classifier = load_model('emotion_little_vgg_3.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get our class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3534 images belonging to 6 classes.\n",
      "{0: 'Angry', 1: 'Fear', 2: 'Happy', 3: 'Neutral', 4: 'Sad', 5: 'Surprise'}\n"
     ]
    }
   ],
   "source": [
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        'validation',\n",
    "        color_mode = 'grayscale',\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "class_labels = validation_generator.class_indices\n",
    "class_labels = {v: k for k, v in class_labels.items()}\n",
    "classes = list(class_labels.values())\n",
    "print(class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's test on some of validation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abuzaid/anaconda3/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:98: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
      "  warnings.warn('grayscale is deprecated. Please use '\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from keras.optimizers import RMSprop, SGD, Adam\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "\n",
    "def draw_test(name, pred, im, true_label):\n",
    "    BLACK = [0,0,0]\n",
    "    expanded_image = cv2.copyMakeBorder(im, 160, 0, 0, 300 ,cv2.BORDER_CONSTANT,value=BLACK)\n",
    "    cv2.putText(expanded_image, \"predited - \"+ pred, (20, 60) , cv2.FONT_HERSHEY_SIMPLEX,1, (0,0,255), 2)\n",
    "    cv2.putText(expanded_image, \"true - \"+ true_label, (20, 120) , cv2.FONT_HERSHEY_SIMPLEX,1, (0,255,0), 2)\n",
    "    cv2.imshow(name, expanded_image)\n",
    "\n",
    "\n",
    "def getRandomImage(path, img_width, img_height):\n",
    "    \"\"\"function loads a random images from a random folder in our test path \"\"\"\n",
    "    folders = list(filter(lambda x: os.path.isdir(os.path.join(path, x)), os.listdir(path)))\n",
    "    random_directory = np.random.randint(0,len(folders))\n",
    "    path_class = folders[random_directory]\n",
    "    file_path = path + path_class\n",
    "    file_names = [f for f in listdir(file_path) if isfile(join(file_path, f))]\n",
    "    random_file_index = np.random.randint(0,len(file_names))\n",
    "    image_name = file_names[random_file_index]\n",
    "    final_path = file_path + \"/\" + image_name\n",
    "    return image.load_img(final_path, target_size = (img_width, img_height),grayscale=True), final_path, path_class\n",
    "\n",
    "# dimensions of our images\n",
    "img_width, img_height = 32, 32\n",
    "\n",
    "# We use a very small learning rate \n",
    "classifier.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = RMSprop(lr = 0.001),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "files = []\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# predicting images\n",
    "for i in range(0, 10):\n",
    "    path = 'validation/' \n",
    "    img, final_path, true_label = getRandomImage(path, img_width, img_height)\n",
    "    files.append(final_path)\n",
    "    true_labels.append(true_label)\n",
    "    x = image.img_to_array(img)\n",
    "    x = x * 1./255\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    images = np.vstack([x])\n",
    "    classes = classifier.predict_classes(images, batch_size = 10)\n",
    "    predictions.append(classes)\n",
    "    \n",
    "for i in range(0, len(files)):\n",
    "    image = cv2.imread((files[i]))\n",
    "    image = cv2.resize(image, None, fx=3, fy=3, interpolation = cv2.INTER_CUBIC)\n",
    "    draw_test(\"Prediction\", class_labels[predictions[i][0]], image, true_labels[i])\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "face_classifier = cv2.CascadeClassifier('./Haarcascades/haarcascade_frontalface_default.xml')\n",
    "\n",
    "def face_detector(img):\n",
    "    # Convert image to grayscale\n",
    "    gray = cv2.cvtColor(img.copy(),cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "    if faces is ():\n",
    "        return (0,0,0,0), np.zeros((32,32), np.uint8), img\n",
    "    \n",
    "    allfaces = []   \n",
    "    rects = []\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_gray = cv2.resize(roi_gray, (32, 32), interpolation = cv2.INTER_AREA)\n",
    "        allfaces.append(roi_gray)\n",
    "        rects.append((x,w,y,h))\n",
    "    return rects, allfaces, img\n",
    "\n",
    "img = cv2.imread(\"rajeev.jpg\")\n",
    "rects, faces, image = face_detector(img)\n",
    "\n",
    "i = 0\n",
    "for face in faces:\n",
    "    roi = face.astype(\"float\") / 255.0\n",
    "    roi = img_to_array(roi)\n",
    "    roi = np.expand_dims(roi, axis=0)\n",
    "\n",
    "    # make a prediction on the ROI, then lookup the class\n",
    "    preds = classifier.predict(roi)[0]\n",
    "    label = class_labels[preds.argmax()]   \n",
    "\n",
    "    #Overlay our detected emotion on our pic\n",
    "    label_position = (rects[i][0] + int((rects[i][1]/2)), abs(rects[i][2] - 10))\n",
    "    i =+ 1\n",
    "    cv2.putText(image, label, label_position , cv2.FONT_HERSHEY_SIMPLEX,1, (0,255,0), 2)\n",
    "    \n",
    "cv2.imshow(\"Emotion Detector\", image)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try this on our webcam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "face_classifier = cv2.CascadeClassifier('./Haarcascades/haarcascade_frontalface_default.xml')\n",
    "\n",
    "def face_detector(img):\n",
    "    # Convert image to grayscale\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "    if faces is ():\n",
    "        return (0,0,0,0), np.zeros((32,32), np.uint8), img\n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "\n",
    "    try:\n",
    "        roi_gray = cv2.resize(roi_gray, (32, 32), interpolation = cv2.INTER_AREA)\n",
    "    except:\n",
    "        return (x,w,y,h), np.zeros((32,32), np.uint8), img\n",
    "    return (x,w,y,h), roi_gray, img\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    rect, face, image = face_detector(frame)\n",
    "    if np.sum([face]) != 0.0:\n",
    "        roi = face.astype(\"float\") / 255.0\n",
    "        roi = img_to_array(roi)\n",
    "        roi = np.expand_dims(roi, axis=0)\n",
    "\n",
    "        # make a prediction on the ROI, then lookup the class\n",
    "        preds = classifier.predict(roi)[0]\n",
    "        label = class_labels[preds.argmax()]  \n",
    "        label_position = (rect[0] + int((rect[1]/2)), rect[2] + 25)\n",
    "        cv2.putText(image, label, label_position , cv2.FONT_HERSHEY_SIMPLEX,2, (0,255,0), 3)\n",
    "    else:\n",
    "        cv2.putText(image, \"No Face Found\", (20, 60) , cv2.FONT_HERSHEY_SIMPLEX,2, (0,255,0), 3)\n",
    "        \n",
    "    cv2.imshow('All', image)\n",
    "    if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
